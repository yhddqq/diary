{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词嵌入（Word Embedding）-词袋模型（CBOW）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 0, 'eat': 1, 'to': 2, 'bananas': 3, 'and': 4, 'love': 5, 'apples': 6}\n",
      "i: 0\n",
      "['I', 'love', 'eat'] tensor([2])\n",
      "tensor([[0, 5, 1]])\n",
      "a torch.Size([1, 3, 5])\n",
      "torch.Size([1, 5])\n",
      "i: 1\n",
      "['love', 'to', 'apples'] tensor([1])\n",
      "tensor([[5, 2, 6]])\n",
      "a torch.Size([1, 3, 5])\n",
      "torch.Size([1, 5])\n",
      "i: 2\n",
      "['to', 'eat', 'and'] tensor([6])\n",
      "tensor([[2, 1, 4]])\n",
      "a torch.Size([1, 3, 5])\n",
      "torch.Size([1, 5])\n",
      "a torch.Size([1, 2, 5])\n",
      "torch.Size([1, 5])\n",
      "1 dict_items([('I', 0), ('eat', 1), ('to', 2), ('bananas', 3), ('and', 4), ('love', 5), ('apples', 6)]) 2\n",
      "2 ['to']\n",
      "预测的中心词: to\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        CBOW 模型的初始化函数。\n",
    "\n",
    "        参数：\n",
    "        vocab_size: 词汇表的大小\n",
    "        embedding_dim: 词嵌入的维度\n",
    "        \"\"\"\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "\n",
    "        参数：\n",
    "        inputs: 输入的上下文单词的索引张量，形状为 (batch_size, context_size)\n",
    "\n",
    "        返回：\n",
    "        输出张量，形状为 (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        print('a', self.embeddings(inputs).shape)\n",
    "        embeds = torch.sum(self.embeddings(inputs), dim=1)  # 对上下文单词的嵌入向量求和\n",
    "        print(embeds.shape)\n",
    "        out = self.linear(embeds)  # 通过线性层得到输出\n",
    "        return out\n",
    "\n",
    "\n",
    "# 示例数据\n",
    "context_size = 4  # 上下文窗口大小\n",
    "vocab_size = 10  # 词汇表大小\n",
    "embedding_dim = 5  # 词嵌入维度\n",
    "sentence = [\"I\", \"love\", \"to\", \"eat\", \"apples\", \"and\", \"bananas\"]\n",
    "word_to_ix = {word: i for i, word in enumerate(set(sentence))}  # 构建单词到索引的映射\n",
    "print(word_to_ix)\n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    \"\"\"\n",
    "    创建上下文向量。\n",
    "\n",
    "    参数：\n",
    "    context: 上下文单词列表\n",
    "    word_to_ix: 单词到索引的映射\n",
    "\n",
    "    返回：\n",
    "    上下文单词的索引张量，形状为 (1, context_size)\n",
    "    \"\"\"\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor([idxs], dtype=torch.long)\n",
    "\n",
    "\n",
    "# 训练 CBOW 模型\n",
    "model = CBOW(vocab_size, embedding_dim)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(1):\n",
    "    total_loss = 0\n",
    "    for i in range(len(sentence) - context_size):\n",
    "        # 输入的上下文\n",
    "        print('i:',i)\n",
    "        context = [sentence[i: i + context_size][j] for j in range(len(sentence[i: i + context_size])) if j!= context_size // 2]\n",
    "        \n",
    "        target = torch.tensor([word_to_ix[sentence[i + context_size // 2]]], dtype=torch.long)\n",
    "        print(context,target)\n",
    "        context_vector = make_context_vector(context, word_to_ix)\n",
    "        print(context_vector)\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(context_vector)\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n",
    "\n",
    "\n",
    "# 测试\n",
    "test_context = [\"love\", \"apples\"]\n",
    "test_context_vector = make_context_vector(test_context, word_to_ix)\n",
    "with torch.no_grad():\n",
    "    log_probs = model(test_context_vector)\n",
    "    predicted_index = torch.argmax(log_probs).item()\n",
    "    print(1,word_to_ix.items(),predicted_index)\n",
    "    print(2, [k for k, v in word_to_ix.items() if v == predicted_index])\n",
    "    predicted_word = [k for k, v in word_to_ix.items() if v == predicted_index][0]\n",
    "    print(f\"预测的中心词: {predicted_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词嵌入（Word Embedding）-跳字模型（Skip-Gram）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "{'I': 0, 'eat': 1, 'to': 2, 'bananas': 3, 'and': 4, 'love': 5, 'apples': 6} [0, 5, 2, 1, 6, 4, 3]\n",
      "----------------------\n",
      "[('I', 'love'), ('I', 'to'), ('I', 'eat'), ('I', 'apples'), ('love', 'I'), ('love', 'to'), ('love', 'eat'), ('love', 'apples'), ('love', 'and'), ('to', 'I'), ('to', 'love'), ('to', 'eat'), ('to', 'apples'), ('to', 'and'), ('to', 'bananas'), ('eat', 'I'), ('eat', 'love'), ('eat', 'to'), ('eat', 'apples'), ('eat', 'and'), ('eat', 'bananas'), ('apples', 'I'), ('apples', 'love'), ('apples', 'to'), ('apples', 'eat'), ('apples', 'and'), ('apples', 'bananas'), ('and', 'love'), ('and', 'to'), ('and', 'eat'), ('and', 'apples'), ('and', 'bananas'), ('bananas', 'to'), ('bananas', 'eat'), ('bananas', 'apples'), ('bananas', 'and')]\n",
      "out torch.Size([1, 7])\n",
      "tensor([0]) tensor([5])\n",
      "out torch.Size([1, 7])\n",
      "tensor([0]) tensor([2])\n",
      "out torch.Size([1, 7])\n",
      "tensor([0]) tensor([1])\n",
      "out torch.Size([1, 7])\n",
      "tensor([0]) tensor([6])\n",
      "out torch.Size([1, 7])\n",
      "tensor([5]) tensor([0])\n",
      "out torch.Size([1, 7])\n",
      "tensor([5]) tensor([2])\n",
      "out torch.Size([1, 7])\n",
      "tensor([5]) tensor([1])\n",
      "out torch.Size([1, 7])\n",
      "tensor([5]) tensor([6])\n",
      "out torch.Size([1, 7])\n",
      "tensor([5]) tensor([4])\n",
      "out torch.Size([1, 7])\n",
      "tensor([2]) tensor([0])\n",
      "out torch.Size([1, 7])\n",
      "tensor([2]) tensor([5])\n",
      "out torch.Size([1, 7])\n",
      "tensor([2]) tensor([1])\n",
      "out torch.Size([1, 7])\n",
      "tensor([2]) tensor([6])\n",
      "out torch.Size([1, 7])\n",
      "tensor([2]) tensor([4])\n",
      "out torch.Size([1, 7])\n",
      "tensor([2]) tensor([3])\n",
      "out torch.Size([1, 7])\n",
      "tensor([1]) tensor([0])\n",
      "out torch.Size([1, 7])\n",
      "tensor([1]) tensor([5])\n",
      "out torch.Size([1, 7])\n",
      "tensor([1]) tensor([2])\n",
      "out torch.Size([1, 7])\n",
      "tensor([1]) tensor([6])\n",
      "out torch.Size([1, 7])\n",
      "tensor([1]) tensor([4])\n",
      "out torch.Size([1, 7])\n",
      "tensor([1]) tensor([3])\n",
      "out torch.Size([1, 7])\n",
      "tensor([6]) tensor([0])\n",
      "out torch.Size([1, 7])\n",
      "tensor([6]) tensor([5])\n",
      "out torch.Size([1, 7])\n",
      "tensor([6]) tensor([2])\n",
      "out torch.Size([1, 7])\n",
      "tensor([6]) tensor([1])\n",
      "out torch.Size([1, 7])\n",
      "tensor([6]) tensor([4])\n",
      "out torch.Size([1, 7])\n",
      "tensor([6]) tensor([3])\n",
      "out torch.Size([1, 7])\n",
      "tensor([4]) tensor([5])\n",
      "out torch.Size([1, 7])\n",
      "tensor([4]) tensor([2])\n",
      "out torch.Size([1, 7])\n",
      "tensor([4]) tensor([1])\n",
      "out torch.Size([1, 7])\n",
      "tensor([4]) tensor([6])\n",
      "out torch.Size([1, 7])\n",
      "tensor([4]) tensor([3])\n",
      "out torch.Size([1, 7])\n",
      "tensor([3]) tensor([2])\n",
      "out torch.Size([1, 7])\n",
      "tensor([3]) tensor([1])\n",
      "out torch.Size([1, 7])\n",
      "tensor([3]) tensor([6])\n",
      "out torch.Size([1, 7])\n",
      "tensor([3]) tensor([4])\n",
      "out torch.Size([1, 7])\n",
      "预测的上下文单词: and\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        SkipGram 模型的初始化函数。\n",
    "\n",
    "        参数:\n",
    "        vocab_size: 词汇表的大小\n",
    "        embedding_dim: 词嵌入的维度\n",
    "        \"\"\"\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_word):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "\n",
    "        参数:\n",
    "        input_word: 输入单词的索引张量，形状为 (batch_size)\n",
    "\n",
    "        返回:\n",
    "        输出张量，形状为 (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        embeds = self.embeddings(input_word)  # 将输入单词的索引转换为嵌入向量\n",
    "        out = self.linear(embeds)  # 通过线性层得到输出\n",
    "        print('out', out.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "def get_skip_gram_pairs(sentence, window_size):\n",
    "    \"\"\"\n",
    "    从句子中获取 Skip-Gram 训练对。\n",
    "\n",
    "    参数:\n",
    "    sentence: 输入的句子，是一个单词列表\n",
    "    window_size: 窗口大小\n",
    "\n",
    "    返回:\n",
    "    输入中心词和上下文单词的索引对列表\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for i in range(len(sentence)):\n",
    "        center_word = sentence[i]\n",
    "        for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
    "            if i!= j:\n",
    "                pairs.append((sentence[i], sentence[j]))\n",
    "    print('----------------------')\n",
    "    print(pairs)\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def prepare_data(sentence):\n",
    "    \"\"\"\n",
    "    准备数据，将单词映射为索引。\n",
    "\n",
    "    参数:\n",
    "    sentence: 输入的句子，是一个单词列表\n",
    "\n",
    "    返回:\n",
    "    单词到索引的映射字典，以及将句子中的单词转换为索引的列表\n",
    "    \"\"\"\n",
    "    word_to_ix = {word: i for i, word in enumerate(set(sentence))}\n",
    "    indices = [word_to_ix[word] for word in sentence]\n",
    "    print('-------------------')\n",
    "    print(word_to_ix, indices)\n",
    "    return word_to_ix, indices\n",
    "\n",
    "\n",
    "# 示例数据\n",
    "sentence = [\"I\", \"love\", \"to\", \"eat\", \"apples\", \"and\", \"bananas\"]\n",
    "window_size = 4\n",
    "vocab_size = len(set(sentence))\n",
    "embedding_dim = 5\n",
    "\n",
    "# 准备数据\n",
    "word_to_ix, indices = prepare_data(sentence)\n",
    "pairs = get_skip_gram_pairs(sentence, window_size)\n",
    "\n",
    "\n",
    "# 初始化 SkipGram 模型\n",
    "model = SkipGram(vocab_size, embedding_dim)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(1):\n",
    "    total_loss = 0\n",
    "    for center_word, context_word in pairs:\n",
    "        center_index = torch.tensor([word_to_ix[center_word]], dtype=torch.long)\n",
    "        context_index = torch.tensor([word_to_ix[context_word]], dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(center_index)\n",
    "        print(center_index,context_index)\n",
    "        loss = loss_function(log_probs, context_index)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n",
    "\n",
    "\n",
    "# 测试\n",
    "test_word = \"love\"\n",
    "test_index = torch.tensor([word_to_ix[test_word]], dtype=torch.long)\n",
    "with torch.no_grad():\n",
    "    log_probs = model(test_index)\n",
    "    predicted_index = torch.argmax(log_probs).item()\n",
    "    predicted_word = [k for k, v in word_to_ix.items() if v == predicted_index][0]\n",
    "    print(f\"预测的上下文单词: {predicted_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词嵌入（Word Embedding）-GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共现矩阵:\n",
      "[[0. 1. 1. 0. 0. 1. 1. 0.]\n",
      " [1. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 0. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 0.]]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "adding a nonzero scalar to a sparse matrix is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 70\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(co_occurrence_matrix\u001b[38;5;241m.\u001b[39mtoarray())\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# 生成词嵌入\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m word_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mglove_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mco_occurrence_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m词嵌入矩阵:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(word_embeddings)\n",
      "Cell \u001b[1;32mIn[2], line 50\u001b[0m, in \u001b[0;36mglove_embedding\u001b[1;34m(co_occurrence_matrix, embedding_dim)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m使用 GloVe 方法生成词嵌入。\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m词嵌入矩阵\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# 加 1 是为了避免 log(0)\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m log_co_occurrence_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[43mco_occurrence_matrix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m)\n\u001b[0;32m     51\u001b[0m U, _, Vt \u001b[38;5;241m=\u001b[39m svds(log_co_occurrence_matrix, embedding_dim)\n\u001b[0;32m     52\u001b[0m word_embeddings \u001b[38;5;241m=\u001b[39m U \u001b[38;5;241m+\u001b[39m Vt\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pypytorch\\lib\\site-packages\\scipy\\sparse\\_base.py:467\u001b[0m, in \u001b[0;36mspmatrix.__add__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;66;03m# Now we would add this scalar to every element.\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madding a nonzero scalar to a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    468\u001b[0m                               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse matrix is not supported\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m isspmatrix(other):\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m other\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape:\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: adding a nonzero scalar to a sparse matrix is not supported"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "\n",
    "def build_co_occurrence_matrix(sentences, vocab_size, window_size):\n",
    "    \"\"\"\n",
    "    构建共现矩阵。\n",
    "\n",
    "    参数:\n",
    "    sentences: 输入的句子列表，每个句子是一个单词列表\n",
    "    vocab_size: 词汇表的大小\n",
    "    window_size: 共现窗口大小\n",
    "\n",
    "    返回:\n",
    "    共现矩阵\n",
    "    \"\"\"\n",
    "    co_occurrence_matrix = lil_matrix((vocab_size, vocab_size), dtype=np.float32)\n",
    "    word_to_index = {}\n",
    "    index = 0\n",
    "    for sentence in sentences:\n",
    "        for i, word in enumerate(sentence):\n",
    "            if word not in word_to_index:\n",
    "                word_to_index[word] = index\n",
    "                index += 1\n",
    "            center_index = word_to_index[word]\n",
    "            for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
    "                if i!= j:\n",
    "                    context_word = sentence[j]\n",
    "                    if context_word not in word_to_index:\n",
    "                        word_to_index[context_word] = index\n",
    "                        index += 1\n",
    "                    context_index = word_to_index[context_word]\n",
    "                    co_occurrence_matrix[center_index, context_index] += 1\n",
    "    return co_occurrence_matrix\n",
    "\n",
    "\n",
    "def glove_embedding(co_occurrence_matrix, embedding_dim):\n",
    "    \"\"\"\n",
    "    使用 GloVe 方法生成词嵌入。\n",
    "\n",
    "    参数:\n",
    "    co_occurrence_matrix: 共现矩阵\n",
    "    embedding_dim: 词嵌入的维度\n",
    "\n",
    "    返回:\n",
    "    词嵌入矩阵\n",
    "    \"\"\"\n",
    "    # 加 1 是为了避免 log(0)\n",
    "    log_co_occurrence_matrix = np.log(co_occurrence_matrix + 1)\n",
    "    U, _, Vt = svds(log_co_occurrence_matrix, embedding_dim)\n",
    "    word_embeddings = U + Vt.T\n",
    "    return word_embeddings\n",
    "\n",
    "\n",
    "# 示例数据\n",
    "sentences = [[\"I\", \"love\", \"to\", \"eat\", \"apples\"],\n",
    "            [\"I\", \"like\", \"bananas\", \"and\", \"apples\"]]\n",
    "vocab_size = len(set([word for sentence in sentences for word in sentence]))\n",
    "window_size = 2\n",
    "embedding_dim = 5\n",
    "\n",
    "# 构建共现矩阵\n",
    "co_occurrence_matrix = build_co_occurrence_matrix(sentences, vocab_size, window_size)\n",
    "print(\"共现矩阵:\")\n",
    "# print(co_occurrence_matrix)\n",
    "print(co_occurrence_matrix.toarray())\n",
    "\n",
    "# 生成词嵌入\n",
    "word_embeddings = glove_embedding(co_occurrence_matrix, embedding_dim)\n",
    "print(\"词嵌入矩阵:\")\n",
    "print(word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 共现矩阵的形状\n",
    "##### 共现矩阵的形状是 (vocab_size, vocab_size)，在这个例子中，vocab_size 是根据输入句子中的不同单词数量计算得到的。对于输入的句子 sentences = [[\"I\", \"love\", \"to\", \"eat\", \"apples\"], [\"I\", \"like\", \"bananas\", \"and\", \"apples\"]]，我们有以下单词：[\"I\", \"love\", \"to\", \"eat\", \"apples\", \"like\", \"bananas\", \"and\"]，所以 vocab_size = 8，因此共现矩阵是一个 8x8 的矩阵。矩阵元素 [i, j] 表示单词 i 和单词 j 的共现次数。\n",
    "\n",
    "##### 词嵌入矩阵的输出\n",
    "##### word_embeddings 是通过对共现矩阵进行奇异值分解（SVD）并处理得到的矩阵，其元素是浮点数，代表每个单词的词嵌入向量的元素。word_embeddings 的形状是 (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### P-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shnh2\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.36.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "{'!': 0, '\"': 1, '#': 2, '$': 3, '%': 4, '&': 5, \"'\": 6, '(': 7, ')': 8, '*': 9, '+': 10, ',': 11, '-': 12, '.': 13, '/': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, ';': 26, '<': 27, '=': 28, '>': 29}\n",
      "50257\n",
      "6\n",
      "Embedding(6, 768)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Accelerator.__init__() got an unexpected keyword argument 'dispatch_batches'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 70\u001b[0m\n\u001b[0;32m     58\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     59\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     60\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m     logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     66\u001b[0m )\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# 定义 Trainer\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp_tuning_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_encodings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[0;32m     78\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pypytorch\\lib\\site-packages\\transformers\\trainer.py:343\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_in_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 343\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_accelerator_and_postprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker \u001b[38;5;241m=\u001b[39m TrainerMemoryTracker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mskip_memory_metrics)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pypytorch\\lib\\site-packages\\transformers\\trainer.py:3882\u001b[0m, in \u001b[0;36mTrainer.create_accelerator_and_postprocess\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3879\u001b[0m gradient_accumulation_plugin \u001b[38;5;241m=\u001b[39m GradientAccumulationPlugin(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgrad_acc_kwargs)\n\u001b[0;32m   3881\u001b[0m \u001b[38;5;66;03m# create accelerator object\u001b[39;00m\n\u001b[1;32m-> 3882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;241m=\u001b[39m \u001b[43mAccelerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdispatch_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3884\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeepspeed_plugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepspeed_plugin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_plugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_accumulation_plugin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3887\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3888\u001b[0m \u001b[38;5;66;03m# some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag\u001b[39;00m\n\u001b[0;32m   3889\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather_for_metrics\n",
      "\u001b[1;31mTypeError\u001b[0m: Accelerator.__init__() got an unexpected keyword argument 'dispatch_batches'"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "\n",
    "# 加载预训练的 GPT-2 模型和 tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "print(model.config)\n",
    "# 获取词汇表的部分结果\n",
    "vocab_part = dict(list(tokenizer.get_vocab().items())[:30])\n",
    "print(vocab_part)\n",
    "print(len(tokenizer.get_vocab()))\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "\n",
    "# 假设我们有训练数据和标签\n",
    "train_texts = [\"This is a positive sentence.\", \"This is a negative sentence.\"]\n",
    "train_labels = [1, 0]\n",
    "\n",
    "\n",
    "# 将文本转换为输入 id\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "\n",
    "# 为 prompt 生成可训练的嵌入参数\n",
    "def create_prompt_embeddings(input_ids):\n",
    "    prompt_embeddings = torch.nn.Embedding(input_ids.size(1), model.config.n_embd)\n",
    "    print(prompt_embeddings.num_embeddings)\n",
    "    return prompt_embeddings\n",
    "\n",
    "\n",
    "# 定义一个简单的分类模型，包含 P-tuning 部分\n",
    "class P_tuningModel(torch.nn.Module):\n",
    "    def __init__(self, base_model, prompt_embeddings):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.prompt_embeddings = prompt_embeddings\n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        prompt_input_ids = torch.arange(self.prompt_embeddings.num_embeddings).unsqueeze(0).to(input_ids.device)\n",
    "        prompt_embeds = self.prompt_embeddings(prompt_input_ids)\n",
    "        input_embeds = self.base_model.transformer.wte(input_ids)\n",
    "        # 拼接 prompt 嵌入和输入嵌入\n",
    "        combined_embeds = torch.cat([prompt_embeds, input_embeds], dim=1)\n",
    "        outputs = self.base_model(inputs_embeds=combined_embeds)\n",
    "        logits = outputs.logits\n",
    "        return logits\n",
    "\n",
    "\n",
    "# 创建可训练的 prompt 嵌入\n",
    "prompt_embeddings = create_prompt_embeddings(train_encodings.input_ids)\n",
    "print(prompt_embeddings)\n",
    "p_tuning_model = P_tuningModel(model, prompt_embeddings)\n",
    "\n",
    "\n",
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "\n",
    "# 定义 Trainer\n",
    "trainer = Trainer(\n",
    "    model=p_tuning_model,\n",
    "    args=training_args,\n",
    "    train_dataset=(train_encodings.input_ids,train_labels)\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15496,    11,   995,     0]])\n",
      "['Hello', ',', 'Ġworld', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "\n",
    "# 加载预训练的 GPT-2 模型和 tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "\n",
    "text = \"Hello, world!\"\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "print(input_ids)\n",
    "\n",
    "\n",
    "# 将输入的 token 编码解码回子词\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 大模型 Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "\n",
    "# 加载预训练的 BERT 模型和 tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "'''\n",
    "# 冻结 BERT 主体的参数\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' not in name:\n",
    "        param.requires_grad = False\n",
    "'''\n",
    "\n",
    "# 假设我们有训练数据和标签\n",
    "train_texts = [\"This is a positive sentence.\", \"This is a negative sentence.\"]\n",
    "train_labels = torch.tensor([1, 0])\n",
    "\n",
    "\n",
    "# 将文本转换为输入 id\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "\n",
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "\n",
    "# 定义 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=(train_encodings.input_ids,train_labels),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 分词算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'an', 'example', 'sentence', '.']\n",
      "[2023, 2003, 2019, 2742, 6251, 1012]\n",
      "{'input_ids': tensor([[ 101, 2023, 2003, 2019, 2742, 6251, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "# 加载预训练的 BERT 分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# 输入文本\n",
    "text = \"This is an example sentence.\"\n",
    "\n",
    "\n",
    "# 分词\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "\n",
    "# 将分词结果转换为输入 id\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "\n",
    "print(input_ids)\n",
    "\n",
    "\n",
    "# 直接将文本转换为输入 id 并添加特殊标记和填充\n",
    "encoded_input = tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 多次输入时使用记忆输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shnh2\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 torch.Size([1, 768])\n",
      "2 torch.Size([1, 768])\n",
      "1 torch.Size([1, 768])\n",
      "2 torch.Size([2, 768])\n",
      "1 torch.Size([1, 768])\n",
      "2 torch.Size([3, 768])\n",
      "3 torch.Size([1, 768])\n",
      "4 torch.Size([2, 768])\n",
      "5 torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# 存储历史输入的嵌入\n",
    "history_embeddings = []\n",
    "\n",
    "\n",
    "def process_input(current_input):\n",
    "    # 对当前输入进行编码\n",
    "    encoded_input = tokenizer(current_input, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    # 获取当前输入的嵌入\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=encoded_input.input_ids, attention_mask=encoded_input.attention_mask)\n",
    "        current_embedding = output.last_hidden_state[:, 0, :]  # 取 [CLS] 标记的嵌入表示\n",
    "        print(1,current_embedding.shape)\n",
    "    # 拼接历史嵌入和当前嵌入\n",
    "    if len(history_embeddings) > 0:\n",
    "        combined_embedding = torch.cat(history_embeddings + [current_embedding], dim=0)\n",
    "    else:\n",
    "        combined_embedding = current_embedding\n",
    "    print(2,combined_embedding.shape)\n",
    "    \n",
    "    history_embeddings.append(current_embedding)\n",
    "    return combined_embedding\n",
    "\n",
    "\n",
    "# 多次输入示例\n",
    "current_input_1 = \"This is an example sentence.\"\n",
    "combined_embedding_1 = process_input(current_input_1)\n",
    "\n",
    "\n",
    "current_input_2 = \"This is another example.\"\n",
    "combined_embedding_2 = process_input(current_input_2)\n",
    "\n",
    "current_input_3 = \"This is another example!.\"\n",
    "combined_embedding_3 = process_input(current_input_3)\n",
    "print(3,combined_embedding_1.shape)\n",
    "print(4,combined_embedding_2.shape)\n",
    "print(5,combined_embedding_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "# 训练 BPE 模型\n",
    "def train_bpe_model(input_file, model_prefix, vocab_size):\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=input_file, \n",
    "        model_prefix=model_prefix, \n",
    "        vocab_size=vocab_size, \n",
    "        model_type='bpe'\n",
    "    )\n",
    "\n",
    "\n",
    "# 加载 BPE 模型\n",
    "def load_bpe_model(model_file):\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(model_file)\n",
    "    return sp\n",
    "\n",
    "\n",
    "# 对文本进行分词\n",
    "def tokenize_with_bpe(sp, text):\n",
    "    return sp.encode_as_pieces(text)\n",
    "\n",
    "\n",
    "# 示例使用\n",
    "input_file = 'corpus.txt'  # 你的语料库文件\n",
    "model_prefix = 'bpe_model'\n",
    "vocab_size = 3000\n",
    "\n",
    "\n",
    "# 训练 BPE 模型\n",
    "train_bpe_model(input_file, model_prefix, vocab_size)\n",
    "\n",
    "\n",
    "# 加载 BPE 模型\n",
    "sp = load_bpe_model(model_prefix + '.model')\n",
    "\n",
    "\n",
    "# 对文本进行分词\n",
    "text = \"This is an example sentence.\"\n",
    "tokens = tokenize_with_bpe(sp, text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "\n",
    "# 精确模式分词\n",
    "def jieba_tokenize_exact(text):\n",
    "    return jieba.cut(text, cut_all=False)\n",
    "\n",
    "\n",
    "# 全模式分词\n",
    "def jieba_tokenize_full(text):\n",
    "    return jieba.cut(text, cut_all=True)\n",
    "\n",
    "\n",
    "# 搜索引擎模式分词\n",
    "def jieba_tokenize_search(text):\n",
    "    return jieba.cut_for_search(text)\n",
    "\n",
    "\n",
    "# 示例使用\n",
    "text = \"我爱自然语言处理\"\n",
    "\n",
    "\n",
    "print(\"精确模式分词:\", list(jieba_tokenize_exact(text)))\n",
    "print(\"全模式分词:\", list(jieba_tokenize_full(text)))\n",
    "print(\"搜索引擎模式分词:\", list(jieba_tokenize_search(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyltp import Segmentor\n",
    "\n",
    "\n",
    "# 加载分词模型\n",
    "LTP_DIR = \"path/to/ltp_data\"  # 请替换为你自己的 LTP 数据路径\n",
    "cws_model_path = os.path.join(LTP_DIR, 'cws.model')\n",
    "segmentor = Segmentor()\n",
    "segmentor.load(cws_model_path)\n",
    "\n",
    "\n",
    "# 分词\n",
    "def ltp_tokenize(text):\n",
    "    return segmentor.segment(text)\n",
    "\n",
    "\n",
    "# 示例使用\n",
    "text = \"我爱自然语言处理\"\n",
    "tokens = ltp_tokenize(text)\n",
    "print(list(tokens))\n",
    "\n",
    "\n",
    "# 释放资源\n",
    "segmentor.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 常见分词算法\n",
    "- 最简单的分词方法，根据空格将文本拆分成单词。\n",
    "- 基于字符的分词（Character Tokenization）\n",
    "原理：\n",
    "将文本拆分成单个字符，每个字符作为一个 token。\n",
    "优点是可以处理未登录词（OOV）问题，因为任何文本都可以拆分成字符。\n",
    "缺点是生成的 token 序列较长，增加了序列长度，可能需要更多的计算资源，且丢失了词语级别的语义信息。\n",
    "- 基于规则的分词（Rule-based Tokenization）\n",
    "原理：\n",
    "使用预定义的规则对文本进行分词，例如对于中文，可以使用基于词典的规则，将文本按词语拆分。\n",
    "- 统计分词（Statistical Tokenization）\n",
    "原理：\n",
    "使用统计信息，如词频、互信息等，将文本拆分成最可能的词语。例如，使用 n-gram 模型，根据 n-gram 的频率信息来确定词的边界。\n",
    "- 子词分词（Subword Tokenization）\n",
    "原理：\n",
    "与 BPE 和 WordPiece 类似，将文本拆分成子词，但可能使用不同的算法。例如，Unigram 语言模型分词是一种子词分词方法\n",
    "\n",
    "##### 对于预训练的大模型，BPE 和 WordPiece 等算法被广泛使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pypytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
